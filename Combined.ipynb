{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o6F6OmFuX6X",
        "outputId": "b07cc238-45c7-4ed7-e4a9-15950073f2b2"
      },
      "source": [
        "# Load the Drive helper and mount\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "# This will prompt for authorization.\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwOBcZ3q6rmk"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "from keras.utils import to_categorical\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3PmvDrB6xox",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "be9c549e-4fab-412a-b5ca-aab51deddfd9"
      },
      "source": [
        "from google.colab import files\r\n",
        "src = list(files.upload().values())[0]\r\n",
        "open('doc3.py','wb').write(src)\r\n",
        "from doc3 import training_doc3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-113a8f2b-f164-49eb-8bf2-f5f0729cea21\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-113a8f2b-f164-49eb-8bf2-f5f0729cea21\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving doc3.py to doc3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Wo0-zd1zlW",
        "outputId": "b4031078-0735-4186-f9b0-949ba1e0dcc8"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JoG_6tb19y_"
      },
      "source": [
        "cleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\r\n",
        "tokens = word_tokenize(cleaned)\r\n",
        "train_len = 3+1\r\n",
        "text_sequences = []\r\n",
        "for i in range(train_len,len(tokens)):\r\n",
        "    seq = tokens[i-train_len:i]\r\n",
        "    text_sequences.append(seq)\r\n",
        "sequences = {}\r\n",
        "count = 1\r\n",
        "for i in range(len(tokens)):\r\n",
        "    if tokens[i] not in sequences:\r\n",
        "        sequences[tokens[i]] = count\r\n",
        "        count += 1\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(text_sequences)\r\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences) \r\n",
        "\r\n",
        "#Collecting some information   \r\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\r\n",
        "\r\n",
        "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\r\n",
        "for i in range(len(sequences)):\r\n",
        "    n_sequences[i] = sequences[i]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drv3uLPD2FV_"
      },
      "source": [
        "train_inputs = n_sequences[:,:-1]\r\n",
        "train_targets = n_sequences[:,-1]\r\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\r\n",
        "seq_len = train_inputs.shape[1]\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XGiC2v7CoV8",
        "outputId": "fe04e3d2-5d44-4367-a9fd-e3b6dcfe010d"
      },
      "source": [
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Embedding\r\n",
        "#model = load_model(\"mymodel.h5\")\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\r\n",
        "model.add(LSTM(50,return_sequences=True))\r\n",
        "model.add(LSTM(50))\r\n",
        "model.add(Dense(50,activation='relu'))\r\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "# compile network\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.fit(train_inputs,train_targets,epochs=500,verbose=1)\r\n",
        "model.save(\"mymodel.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 3, 3)              8325      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 3, 50)             10800     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2775)              141525    \n",
            "=================================================================\n",
            "Total params: 183,400\n",
            "Trainable params: 183,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 7.1656 - accuracy: 0.0350\n",
            "Epoch 2/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.7225 - accuracy: 0.0399\n",
            "Epoch 3/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.6413 - accuracy: 0.0399\n",
            "Epoch 4/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.5911 - accuracy: 0.0399\n",
            "Epoch 5/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.5561 - accuracy: 0.0399\n",
            "Epoch 6/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.5166 - accuracy: 0.0400\n",
            "Epoch 7/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.4723 - accuracy: 0.0397\n",
            "Epoch 8/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.4218 - accuracy: 0.0399\n",
            "Epoch 9/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.3420 - accuracy: 0.0427\n",
            "Epoch 10/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.2745 - accuracy: 0.0447\n",
            "Epoch 11/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.2067 - accuracy: 0.0476\n",
            "Epoch 12/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.1397 - accuracy: 0.0539\n",
            "Epoch 13/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.0731 - accuracy: 0.0567\n",
            "Epoch 14/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 6.0041 - accuracy: 0.0571\n",
            "Epoch 15/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.9321 - accuracy: 0.0626\n",
            "Epoch 16/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.8595 - accuracy: 0.0648\n",
            "Epoch 17/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.7905 - accuracy: 0.0706\n",
            "Epoch 18/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.7160 - accuracy: 0.0731\n",
            "Epoch 19/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.6411 - accuracy: 0.0740\n",
            "Epoch 20/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.5714 - accuracy: 0.0769\n",
            "Epoch 21/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.5001 - accuracy: 0.0785\n",
            "Epoch 22/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.4325 - accuracy: 0.0799\n",
            "Epoch 23/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.3668 - accuracy: 0.0832\n",
            "Epoch 24/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.3006 - accuracy: 0.0866\n",
            "Epoch 25/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.2360 - accuracy: 0.0896\n",
            "Epoch 26/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.1716 - accuracy: 0.0928\n",
            "Epoch 27/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.1037 - accuracy: 0.0957\n",
            "Epoch 28/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 5.0351 - accuracy: 0.1001\n",
            "Epoch 29/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.9650 - accuracy: 0.0999\n",
            "Epoch 30/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.8844 - accuracy: 0.1042\n",
            "Epoch 31/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.8087 - accuracy: 0.1045\n",
            "Epoch 32/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.7433 - accuracy: 0.1092\n",
            "Epoch 33/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.6606 - accuracy: 0.1091\n",
            "Epoch 34/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.5861 - accuracy: 0.1126\n",
            "Epoch 35/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.5054 - accuracy: 0.1172\n",
            "Epoch 36/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.4314 - accuracy: 0.1165\n",
            "Epoch 37/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.3564 - accuracy: 0.1191\n",
            "Epoch 38/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.2811 - accuracy: 0.1256\n",
            "Epoch 39/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.2239 - accuracy: 0.1258\n",
            "Epoch 40/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.1532 - accuracy: 0.1301\n",
            "Epoch 41/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.0889 - accuracy: 0.1345\n",
            "Epoch 42/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 4.0341 - accuracy: 0.1398\n",
            "Epoch 43/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.9805 - accuracy: 0.1434\n",
            "Epoch 44/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.9299 - accuracy: 0.1504\n",
            "Epoch 45/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.8752 - accuracy: 0.1536\n",
            "Epoch 46/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.8276 - accuracy: 0.1593\n",
            "Epoch 47/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.7772 - accuracy: 0.1715\n",
            "Epoch 48/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.7385 - accuracy: 0.1724\n",
            "Epoch 49/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.6902 - accuracy: 0.1787\n",
            "Epoch 50/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.6421 - accuracy: 0.1874\n",
            "Epoch 51/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.5964 - accuracy: 0.1920\n",
            "Epoch 52/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.5502 - accuracy: 0.1973\n",
            "Epoch 53/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.5097 - accuracy: 0.2054\n",
            "Epoch 54/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.4667 - accuracy: 0.2098\n",
            "Epoch 55/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.4267 - accuracy: 0.2196\n",
            "Epoch 56/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.3817 - accuracy: 0.2214\n",
            "Epoch 57/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.3413 - accuracy: 0.2318\n",
            "Epoch 58/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.2923 - accuracy: 0.2406\n",
            "Epoch 59/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.2647 - accuracy: 0.2460\n",
            "Epoch 60/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.2210 - accuracy: 0.2567\n",
            "Epoch 61/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.1943 - accuracy: 0.2595\n",
            "Epoch 62/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.1565 - accuracy: 0.2669\n",
            "Epoch 63/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.1191 - accuracy: 0.2763\n",
            "Epoch 64/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.0929 - accuracy: 0.2796\n",
            "Epoch 65/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.0610 - accuracy: 0.2829\n",
            "Epoch 66/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 3.0301 - accuracy: 0.2918\n",
            "Epoch 67/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.9937 - accuracy: 0.3011\n",
            "Epoch 68/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.9712 - accuracy: 0.2999\n",
            "Epoch 69/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.9464 - accuracy: 0.3069\n",
            "Epoch 70/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.9109 - accuracy: 0.3135\n",
            "Epoch 71/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.8856 - accuracy: 0.3150\n",
            "Epoch 72/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.8643 - accuracy: 0.3222\n",
            "Epoch 73/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.8382 - accuracy: 0.3288\n",
            "Epoch 74/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.8076 - accuracy: 0.3322\n",
            "Epoch 75/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.7810 - accuracy: 0.3420\n",
            "Epoch 76/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.7576 - accuracy: 0.3441\n",
            "Epoch 77/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.7381 - accuracy: 0.3509\n",
            "Epoch 78/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.7151 - accuracy: 0.3538\n",
            "Epoch 79/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.6947 - accuracy: 0.3541\n",
            "Epoch 80/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.6757 - accuracy: 0.3593\n",
            "Epoch 81/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.6442 - accuracy: 0.3714\n",
            "Epoch 82/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.6238 - accuracy: 0.3715\n",
            "Epoch 83/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.6076 - accuracy: 0.3730\n",
            "Epoch 84/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.5936 - accuracy: 0.3750\n",
            "Epoch 85/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.5726 - accuracy: 0.3763\n",
            "Epoch 86/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.5451 - accuracy: 0.3871\n",
            "Epoch 87/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.5259 - accuracy: 0.3936\n",
            "Epoch 88/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.5150 - accuracy: 0.3927\n",
            "Epoch 89/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4845 - accuracy: 0.4031\n",
            "Epoch 90/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4754 - accuracy: 0.4016\n",
            "Epoch 91/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4660 - accuracy: 0.4043\n",
            "Epoch 92/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4443 - accuracy: 0.4052\n",
            "Epoch 93/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4178 - accuracy: 0.4119\n",
            "Epoch 94/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.4202 - accuracy: 0.4110\n",
            "Epoch 95/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3913 - accuracy: 0.4218\n",
            "Epoch 96/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3780 - accuracy: 0.4237\n",
            "Epoch 97/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3537 - accuracy: 0.4296\n",
            "Epoch 98/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3384 - accuracy: 0.4311\n",
            "Epoch 99/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3286 - accuracy: 0.4311\n",
            "Epoch 100/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.3188 - accuracy: 0.4336\n",
            "Epoch 101/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2965 - accuracy: 0.4441\n",
            "Epoch 102/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2807 - accuracy: 0.4407\n",
            "Epoch 103/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2697 - accuracy: 0.4450\n",
            "Epoch 104/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2551 - accuracy: 0.4494\n",
            "Epoch 105/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2554 - accuracy: 0.4471\n",
            "Epoch 106/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2218 - accuracy: 0.4543\n",
            "Epoch 107/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.2094 - accuracy: 0.4559\n",
            "Epoch 108/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1934 - accuracy: 0.4634\n",
            "Epoch 109/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1867 - accuracy: 0.4629\n",
            "Epoch 110/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1702 - accuracy: 0.4687\n",
            "Epoch 111/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1621 - accuracy: 0.4674\n",
            "Epoch 112/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1373 - accuracy: 0.4716\n",
            "Epoch 113/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1235 - accuracy: 0.4736\n",
            "Epoch 114/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1180 - accuracy: 0.4828\n",
            "Epoch 115/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.1145 - accuracy: 0.4797\n",
            "Epoch 116/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0977 - accuracy: 0.4842\n",
            "Epoch 117/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0955 - accuracy: 0.4809\n",
            "Epoch 118/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0752 - accuracy: 0.4822\n",
            "Epoch 119/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0656 - accuracy: 0.4916\n",
            "Epoch 120/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0595 - accuracy: 0.4873\n",
            "Epoch 121/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0447 - accuracy: 0.4889\n",
            "Epoch 122/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0299 - accuracy: 0.4974\n",
            "Epoch 123/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0215 - accuracy: 0.4999\n",
            "Epoch 124/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 2.0058 - accuracy: 0.4996\n",
            "Epoch 125/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9883 - accuracy: 0.5105\n",
            "Epoch 126/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9918 - accuracy: 0.5037\n",
            "Epoch 127/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9841 - accuracy: 0.5039\n",
            "Epoch 128/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9718 - accuracy: 0.5100\n",
            "Epoch 129/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9437 - accuracy: 0.5130\n",
            "Epoch 130/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9332 - accuracy: 0.5235\n",
            "Epoch 131/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9182 - accuracy: 0.5266\n",
            "Epoch 132/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9212 - accuracy: 0.5190\n",
            "Epoch 133/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9195 - accuracy: 0.5211\n",
            "Epoch 134/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.9160 - accuracy: 0.5231\n",
            "Epoch 135/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8991 - accuracy: 0.5277\n",
            "Epoch 136/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8782 - accuracy: 0.5323\n",
            "Epoch 137/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8766 - accuracy: 0.5348\n",
            "Epoch 138/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8706 - accuracy: 0.5346\n",
            "Epoch 139/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8678 - accuracy: 0.5320\n",
            "Epoch 140/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8454 - accuracy: 0.5419\n",
            "Epoch 141/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8400 - accuracy: 0.5393\n",
            "Epoch 142/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8304 - accuracy: 0.5395\n",
            "Epoch 143/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8188 - accuracy: 0.5431\n",
            "Epoch 144/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8151 - accuracy: 0.5424\n",
            "Epoch 145/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8104 - accuracy: 0.5419\n",
            "Epoch 146/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8040 - accuracy: 0.5488\n",
            "Epoch 147/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.8008 - accuracy: 0.5509\n",
            "Epoch 148/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7863 - accuracy: 0.5496\n",
            "Epoch 149/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7710 - accuracy: 0.5547\n",
            "Epoch 150/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7633 - accuracy: 0.5624\n",
            "Epoch 151/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7465 - accuracy: 0.5622\n",
            "Epoch 152/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7458 - accuracy: 0.5636\n",
            "Epoch 153/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7403 - accuracy: 0.5657\n",
            "Epoch 154/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7333 - accuracy: 0.5596\n",
            "Epoch 155/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.7204 - accuracy: 0.5664\n",
            "Epoch 156/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 1.7314 - accuracy: 0.5611\n",
            "Epoch 157/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 1.7171 - accuracy: 0.5662\n",
            "Epoch 158/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 1.7107 - accuracy: 0.5724\n",
            "Epoch 159/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 1.6880 - accuracy: 0.5766\n",
            "Epoch 160/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 1.6859 - accuracy: 0.5749\n",
            "Epoch 161/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6836 - accuracy: 0.5770\n",
            "Epoch 162/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6713 - accuracy: 0.5762\n",
            "Epoch 163/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6730 - accuracy: 0.5793\n",
            "Epoch 164/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6535 - accuracy: 0.5803\n",
            "Epoch 165/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6511 - accuracy: 0.5822\n",
            "Epoch 166/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6660 - accuracy: 0.5748\n",
            "Epoch 167/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6540 - accuracy: 0.5849\n",
            "Epoch 168/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6326 - accuracy: 0.5901\n",
            "Epoch 169/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6269 - accuracy: 0.5892\n",
            "Epoch 170/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6085 - accuracy: 0.5912\n",
            "Epoch 171/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6083 - accuracy: 0.5928\n",
            "Epoch 172/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5947 - accuracy: 0.5980\n",
            "Epoch 173/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5998 - accuracy: 0.5933\n",
            "Epoch 174/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.6009 - accuracy: 0.5905\n",
            "Epoch 175/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5862 - accuracy: 0.5988\n",
            "Epoch 176/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5743 - accuracy: 0.6001\n",
            "Epoch 177/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5646 - accuracy: 0.5997\n",
            "Epoch 178/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5573 - accuracy: 0.6021\n",
            "Epoch 179/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5572 - accuracy: 0.6099\n",
            "Epoch 180/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5565 - accuracy: 0.6015\n",
            "Epoch 181/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5513 - accuracy: 0.5993\n",
            "Epoch 182/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5627 - accuracy: 0.5976\n",
            "Epoch 183/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5405 - accuracy: 0.6046\n",
            "Epoch 184/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5310 - accuracy: 0.6117\n",
            "Epoch 185/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5153 - accuracy: 0.6145\n",
            "Epoch 186/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5049 - accuracy: 0.6132\n",
            "Epoch 187/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5176 - accuracy: 0.6097\n",
            "Epoch 188/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5229 - accuracy: 0.6094\n",
            "Epoch 189/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.5003 - accuracy: 0.6195\n",
            "Epoch 190/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4895 - accuracy: 0.6175\n",
            "Epoch 191/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4938 - accuracy: 0.6132\n",
            "Epoch 192/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4812 - accuracy: 0.6193\n",
            "Epoch 193/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4605 - accuracy: 0.6252\n",
            "Epoch 194/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4694 - accuracy: 0.6247\n",
            "Epoch 195/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4590 - accuracy: 0.6311\n",
            "Epoch 196/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4588 - accuracy: 0.6231\n",
            "Epoch 197/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4531 - accuracy: 0.6238\n",
            "Epoch 198/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4612 - accuracy: 0.6223\n",
            "Epoch 199/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4576 - accuracy: 0.6205\n",
            "Epoch 200/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4600 - accuracy: 0.6250\n",
            "Epoch 201/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4219 - accuracy: 0.6337\n",
            "Epoch 202/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4196 - accuracy: 0.6318\n",
            "Epoch 203/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4182 - accuracy: 0.6342\n",
            "Epoch 204/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4223 - accuracy: 0.6343\n",
            "Epoch 205/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4083 - accuracy: 0.6367\n",
            "Epoch 206/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4170 - accuracy: 0.6346\n",
            "Epoch 207/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4029 - accuracy: 0.6369\n",
            "Epoch 208/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3960 - accuracy: 0.6429\n",
            "Epoch 209/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3788 - accuracy: 0.6440\n",
            "Epoch 210/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3903 - accuracy: 0.6419\n",
            "Epoch 211/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.4030 - accuracy: 0.6334\n",
            "Epoch 212/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3848 - accuracy: 0.6452\n",
            "Epoch 213/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3678 - accuracy: 0.6436\n",
            "Epoch 214/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3628 - accuracy: 0.6512\n",
            "Epoch 215/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3694 - accuracy: 0.6436\n",
            "Epoch 216/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3521 - accuracy: 0.6509\n",
            "Epoch 217/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3414 - accuracy: 0.6529\n",
            "Epoch 218/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3421 - accuracy: 0.6536\n",
            "Epoch 219/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3585 - accuracy: 0.6482\n",
            "Epoch 220/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3429 - accuracy: 0.6517\n",
            "Epoch 221/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3378 - accuracy: 0.6567\n",
            "Epoch 222/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3380 - accuracy: 0.6549\n",
            "Epoch 223/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3344 - accuracy: 0.6527\n",
            "Epoch 224/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3108 - accuracy: 0.6605\n",
            "Epoch 225/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3048 - accuracy: 0.6639\n",
            "Epoch 226/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.3036 - accuracy: 0.6629\n",
            "Epoch 227/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2940 - accuracy: 0.6665\n",
            "Epoch 228/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2989 - accuracy: 0.6621\n",
            "Epoch 229/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2835 - accuracy: 0.6651\n",
            "Epoch 230/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2985 - accuracy: 0.6615\n",
            "Epoch 231/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2976 - accuracy: 0.6638\n",
            "Epoch 232/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2868 - accuracy: 0.6618\n",
            "Epoch 233/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2853 - accuracy: 0.6647\n",
            "Epoch 234/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2621 - accuracy: 0.6743\n",
            "Epoch 235/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2619 - accuracy: 0.6723\n",
            "Epoch 236/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2501 - accuracy: 0.6782\n",
            "Epoch 237/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2512 - accuracy: 0.6738\n",
            "Epoch 238/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2734 - accuracy: 0.6658\n",
            "Epoch 239/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2769 - accuracy: 0.6668\n",
            "Epoch 240/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2631 - accuracy: 0.6724\n",
            "Epoch 241/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2371 - accuracy: 0.6782\n",
            "Epoch 242/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2414 - accuracy: 0.6776\n",
            "Epoch 243/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2438 - accuracy: 0.6760\n",
            "Epoch 244/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2357 - accuracy: 0.6761\n",
            "Epoch 245/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2266 - accuracy: 0.6850\n",
            "Epoch 246/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2120 - accuracy: 0.6839\n",
            "Epoch 247/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2081 - accuracy: 0.6900\n",
            "Epoch 248/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2275 - accuracy: 0.6817\n",
            "Epoch 249/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2038 - accuracy: 0.6834\n",
            "Epoch 250/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1952 - accuracy: 0.6846\n",
            "Epoch 251/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1939 - accuracy: 0.6856\n",
            "Epoch 252/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2115 - accuracy: 0.6850\n",
            "Epoch 253/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1985 - accuracy: 0.6877\n",
            "Epoch 254/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1955 - accuracy: 0.6846\n",
            "Epoch 255/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.2049 - accuracy: 0.6872\n",
            "Epoch 256/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1909 - accuracy: 0.6875\n",
            "Epoch 257/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1753 - accuracy: 0.6936\n",
            "Epoch 258/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1637 - accuracy: 0.6990\n",
            "Epoch 259/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1684 - accuracy: 0.6927\n",
            "Epoch 260/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1744 - accuracy: 0.6891\n",
            "Epoch 261/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1638 - accuracy: 0.6973\n",
            "Epoch 262/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1536 - accuracy: 0.6981\n",
            "Epoch 263/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1602 - accuracy: 0.6992\n",
            "Epoch 264/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1458 - accuracy: 0.7007\n",
            "Epoch 265/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1557 - accuracy: 0.6965\n",
            "Epoch 266/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1487 - accuracy: 0.6989\n",
            "Epoch 267/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1448 - accuracy: 0.6993\n",
            "Epoch 268/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1463 - accuracy: 0.6992\n",
            "Epoch 269/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1568 - accuracy: 0.6940\n",
            "Epoch 270/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1302 - accuracy: 0.6992\n",
            "Epoch 271/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1285 - accuracy: 0.7059\n",
            "Epoch 272/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1253 - accuracy: 0.7063\n",
            "Epoch 273/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1017 - accuracy: 0.7144\n",
            "Epoch 274/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1215 - accuracy: 0.7056\n",
            "Epoch 275/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1173 - accuracy: 0.7043\n",
            "Epoch 276/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1056 - accuracy: 0.7073\n",
            "Epoch 277/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0909 - accuracy: 0.7110\n",
            "Epoch 278/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1068 - accuracy: 0.7094\n",
            "Epoch 279/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1149 - accuracy: 0.7060\n",
            "Epoch 280/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1161 - accuracy: 0.7019\n",
            "Epoch 281/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.1086 - accuracy: 0.7060\n",
            "Epoch 282/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0797 - accuracy: 0.7139\n",
            "Epoch 283/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0605 - accuracy: 0.7252\n",
            "Epoch 284/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0636 - accuracy: 0.7190\n",
            "Epoch 285/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0657 - accuracy: 0.7174\n",
            "Epoch 286/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0737 - accuracy: 0.7151\n",
            "Epoch 287/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0818 - accuracy: 0.7108\n",
            "Epoch 288/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0875 - accuracy: 0.7110\n",
            "Epoch 289/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0656 - accuracy: 0.7176\n",
            "Epoch 290/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0733 - accuracy: 0.7161\n",
            "Epoch 291/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0472 - accuracy: 0.7243\n",
            "Epoch 292/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0265 - accuracy: 0.7299\n",
            "Epoch 293/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0239 - accuracy: 0.7345\n",
            "Epoch 294/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0368 - accuracy: 0.7255\n",
            "Epoch 295/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0489 - accuracy: 0.7223\n",
            "Epoch 296/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0346 - accuracy: 0.7225\n",
            "Epoch 297/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0356 - accuracy: 0.7273\n",
            "Epoch 298/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0470 - accuracy: 0.7223\n",
            "Epoch 299/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0434 - accuracy: 0.7215\n",
            "Epoch 300/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0364 - accuracy: 0.7199\n",
            "Epoch 301/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0357 - accuracy: 0.7249\n",
            "Epoch 302/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0199 - accuracy: 0.7290\n",
            "Epoch 303/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9872 - accuracy: 0.7380\n",
            "Epoch 304/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9814 - accuracy: 0.7394\n",
            "Epoch 305/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9946 - accuracy: 0.7365\n",
            "Epoch 306/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0084 - accuracy: 0.7324\n",
            "Epoch 307/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0157 - accuracy: 0.7301\n",
            "Epoch 308/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9920 - accuracy: 0.7399\n",
            "Epoch 309/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0042 - accuracy: 0.7330\n",
            "Epoch 310/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0085 - accuracy: 0.7288\n",
            "Epoch 311/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0063 - accuracy: 0.7292\n",
            "Epoch 312/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9839 - accuracy: 0.7371\n",
            "Epoch 313/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9759 - accuracy: 0.7428\n",
            "Epoch 314/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9689 - accuracy: 0.7442\n",
            "Epoch 315/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9839 - accuracy: 0.7381\n",
            "Epoch 316/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9872 - accuracy: 0.7395\n",
            "Epoch 317/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 1.0048 - accuracy: 0.7301\n",
            "Epoch 318/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9688 - accuracy: 0.7363\n",
            "Epoch 319/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9573 - accuracy: 0.7424\n",
            "Epoch 320/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9310 - accuracy: 0.7518\n",
            "Epoch 321/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9287 - accuracy: 0.7531\n",
            "Epoch 322/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9432 - accuracy: 0.7491\n",
            "Epoch 323/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9481 - accuracy: 0.7477\n",
            "Epoch 324/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9624 - accuracy: 0.7411\n",
            "Epoch 325/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9565 - accuracy: 0.7437\n",
            "Epoch 326/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9499 - accuracy: 0.7504\n",
            "Epoch 327/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9405 - accuracy: 0.7471\n",
            "Epoch 328/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9245 - accuracy: 0.7514\n",
            "Epoch 329/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9251 - accuracy: 0.7515\n",
            "Epoch 330/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9145 - accuracy: 0.7584\n",
            "Epoch 331/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.9424 - accuracy: 0.7435\n",
            "Epoch 332/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.9482 - accuracy: 0.7420\n",
            "Epoch 333/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.9205 - accuracy: 0.7557\n",
            "Epoch 334/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.8999 - accuracy: 0.7626\n",
            "Epoch 335/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.8937 - accuracy: 0.7640\n",
            "Epoch 336/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8945 - accuracy: 0.7636\n",
            "Epoch 337/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9388 - accuracy: 0.7489\n",
            "Epoch 338/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9424 - accuracy: 0.7444\n",
            "Epoch 339/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9261 - accuracy: 0.7548\n",
            "Epoch 340/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8924 - accuracy: 0.7611\n",
            "Epoch 341/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8759 - accuracy: 0.7671\n",
            "Epoch 342/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8865 - accuracy: 0.7625\n",
            "Epoch 343/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9168 - accuracy: 0.7547\n",
            "Epoch 344/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9050 - accuracy: 0.7558\n",
            "Epoch 345/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.9004 - accuracy: 0.7599\n",
            "Epoch 346/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8828 - accuracy: 0.7608\n",
            "Epoch 347/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8727 - accuracy: 0.7629\n",
            "Epoch 348/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8553 - accuracy: 0.7691\n",
            "Epoch 349/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8862 - accuracy: 0.7607\n",
            "Epoch 350/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8586 - accuracy: 0.7704\n",
            "Epoch 351/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8685 - accuracy: 0.7674\n",
            "Epoch 352/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8931 - accuracy: 0.7599\n",
            "Epoch 353/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8780 - accuracy: 0.7597\n",
            "Epoch 354/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8874 - accuracy: 0.7575\n",
            "Epoch 355/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8579 - accuracy: 0.7709\n",
            "Epoch 356/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8605 - accuracy: 0.7678\n",
            "Epoch 357/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8636 - accuracy: 0.7657\n",
            "Epoch 358/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.8644 - accuracy: 0.7674\n",
            "Epoch 359/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8428 - accuracy: 0.7747\n",
            "Epoch 360/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8353 - accuracy: 0.7736\n",
            "Epoch 361/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8246 - accuracy: 0.7801\n",
            "Epoch 362/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8268 - accuracy: 0.7777\n",
            "Epoch 363/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8371 - accuracy: 0.7724\n",
            "Epoch 364/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8567 - accuracy: 0.7683\n",
            "Epoch 365/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8485 - accuracy: 0.7730\n",
            "Epoch 366/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8422 - accuracy: 0.7724\n",
            "Epoch 367/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8253 - accuracy: 0.7811\n",
            "Epoch 368/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8142 - accuracy: 0.7844\n",
            "Epoch 369/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8234 - accuracy: 0.7793\n",
            "Epoch 370/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8257 - accuracy: 0.7754\n",
            "Epoch 371/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8124 - accuracy: 0.7805\n",
            "Epoch 372/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8086 - accuracy: 0.7806\n",
            "Epoch 373/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7923 - accuracy: 0.7870\n",
            "Epoch 374/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8367 - accuracy: 0.7731\n",
            "Epoch 375/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8304 - accuracy: 0.7756\n",
            "Epoch 376/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8093 - accuracy: 0.7826\n",
            "Epoch 377/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7960 - accuracy: 0.7850\n",
            "Epoch 378/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7925 - accuracy: 0.7867\n",
            "Epoch 379/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8270 - accuracy: 0.7745\n",
            "Epoch 380/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8229 - accuracy: 0.7774\n",
            "Epoch 381/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7885 - accuracy: 0.7910\n",
            "Epoch 382/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7884 - accuracy: 0.7902\n",
            "Epoch 383/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7866 - accuracy: 0.7877\n",
            "Epoch 384/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7785 - accuracy: 0.7950\n",
            "Epoch 385/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7817 - accuracy: 0.7898\n",
            "Epoch 386/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7835 - accuracy: 0.7892\n",
            "Epoch 387/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8105 - accuracy: 0.7805\n",
            "Epoch 388/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.8179 - accuracy: 0.7774\n",
            "Epoch 389/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7927 - accuracy: 0.7858\n",
            "Epoch 390/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7734 - accuracy: 0.7867\n",
            "Epoch 391/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7711 - accuracy: 0.7914\n",
            "Epoch 392/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7505 - accuracy: 0.8024\n",
            "Epoch 393/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7504 - accuracy: 0.7986\n",
            "Epoch 394/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7463 - accuracy: 0.7980\n",
            "Epoch 395/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7392 - accuracy: 0.7988\n",
            "Epoch 396/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7675 - accuracy: 0.7913\n",
            "Epoch 397/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7932 - accuracy: 0.7851\n",
            "Epoch 398/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7976 - accuracy: 0.7797\n",
            "Epoch 399/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7615 - accuracy: 0.7892\n",
            "Epoch 400/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7334 - accuracy: 0.8006\n",
            "Epoch 401/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7134 - accuracy: 0.8031\n",
            "Epoch 402/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7265 - accuracy: 0.8027\n",
            "Epoch 403/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7846 - accuracy: 0.7879\n",
            "Epoch 404/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7787 - accuracy: 0.7854\n",
            "Epoch 405/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7415 - accuracy: 0.8007\n",
            "Epoch 406/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7288 - accuracy: 0.8040\n",
            "Epoch 407/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7229 - accuracy: 0.8040\n",
            "Epoch 408/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7361 - accuracy: 0.8026\n",
            "Epoch 409/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7358 - accuracy: 0.8027\n",
            "Epoch 410/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7317 - accuracy: 0.8006\n",
            "Epoch 411/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7024 - accuracy: 0.8121\n",
            "Epoch 412/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7187 - accuracy: 0.8018\n",
            "Epoch 413/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7338 - accuracy: 0.7999\n",
            "Epoch 414/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7369 - accuracy: 0.8018\n",
            "Epoch 415/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7099 - accuracy: 0.8062\n",
            "Epoch 416/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7438 - accuracy: 0.7979\n",
            "Epoch 417/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7360 - accuracy: 0.7960\n",
            "Epoch 418/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7304 - accuracy: 0.8007\n",
            "Epoch 419/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7170 - accuracy: 0.8046\n",
            "Epoch 420/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7070 - accuracy: 0.8086\n",
            "Epoch 421/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6792 - accuracy: 0.8194\n",
            "Epoch 422/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6947 - accuracy: 0.8095\n",
            "Epoch 423/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7108 - accuracy: 0.8060\n",
            "Epoch 424/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6888 - accuracy: 0.8115\n",
            "Epoch 425/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6727 - accuracy: 0.8178\n",
            "Epoch 426/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6946 - accuracy: 0.8078\n",
            "Epoch 427/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7069 - accuracy: 0.8059\n",
            "Epoch 428/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.7219 - accuracy: 0.8028\n",
            "Epoch 429/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6866 - accuracy: 0.8101\n",
            "Epoch 430/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6817 - accuracy: 0.8128\n",
            "Epoch 431/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6905 - accuracy: 0.8133\n",
            "Epoch 432/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6914 - accuracy: 0.8092\n",
            "Epoch 433/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6682 - accuracy: 0.8201\n",
            "Epoch 434/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.8195\n",
            "Epoch 435/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6820 - accuracy: 0.8116\n",
            "Epoch 436/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6952 - accuracy: 0.8109\n",
            "Epoch 437/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6733 - accuracy: 0.8150\n",
            "Epoch 438/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6660 - accuracy: 0.8216\n",
            "Epoch 439/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6733 - accuracy: 0.8143\n",
            "Epoch 440/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6522 - accuracy: 0.8208\n",
            "Epoch 441/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6560 - accuracy: 0.8172\n",
            "Epoch 442/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6410 - accuracy: 0.8241\n",
            "Epoch 443/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6618 - accuracy: 0.8169\n",
            "Epoch 444/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6734 - accuracy: 0.8188\n",
            "Epoch 445/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6750 - accuracy: 0.8135\n",
            "Epoch 446/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6544 - accuracy: 0.8205\n",
            "Epoch 447/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6397 - accuracy: 0.8231\n",
            "Epoch 448/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6600 - accuracy: 0.8184\n",
            "Epoch 449/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6580 - accuracy: 0.8219\n",
            "Epoch 450/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6502 - accuracy: 0.8234\n",
            "Epoch 451/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6580 - accuracy: 0.8175\n",
            "Epoch 452/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6405 - accuracy: 0.8218\n",
            "Epoch 453/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6402 - accuracy: 0.8237\n",
            "Epoch 454/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6337 - accuracy: 0.8270\n",
            "Epoch 455/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6367 - accuracy: 0.8238\n",
            "Epoch 456/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6322 - accuracy: 0.8254\n",
            "Epoch 457/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6335 - accuracy: 0.8267\n",
            "Epoch 458/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6294 - accuracy: 0.8261\n",
            "Epoch 459/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6252 - accuracy: 0.8275\n",
            "Epoch 460/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6141 - accuracy: 0.8286\n",
            "Epoch 461/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6289 - accuracy: 0.8273\n",
            "Epoch 462/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6157 - accuracy: 0.8292\n",
            "Epoch 463/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5918 - accuracy: 0.8389\n",
            "Epoch 464/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6051 - accuracy: 0.8345\n",
            "Epoch 465/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6360 - accuracy: 0.8235\n",
            "Epoch 466/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6693 - accuracy: 0.8096\n",
            "Epoch 467/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6286 - accuracy: 0.8259\n",
            "Epoch 468/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6089 - accuracy: 0.8327\n",
            "Epoch 469/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5886 - accuracy: 0.8377\n",
            "Epoch 470/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5790 - accuracy: 0.8405\n",
            "Epoch 471/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5849 - accuracy: 0.8361\n",
            "Epoch 472/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6136 - accuracy: 0.8258\n",
            "Epoch 473/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6101 - accuracy: 0.8297\n",
            "Epoch 474/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6025 - accuracy: 0.8291\n",
            "Epoch 475/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6323 - accuracy: 0.8250\n",
            "Epoch 476/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.6340 - accuracy: 0.8230\n",
            "Epoch 477/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5760 - accuracy: 0.8418\n",
            "Epoch 478/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5500 - accuracy: 0.8475\n",
            "Epoch 479/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5531 - accuracy: 0.8464\n",
            "Epoch 480/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5751 - accuracy: 0.8403\n",
            "Epoch 481/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5836 - accuracy: 0.8359\n",
            "Epoch 482/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5898 - accuracy: 0.8369\n",
            "Epoch 483/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5910 - accuracy: 0.8372\n",
            "Epoch 484/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5911 - accuracy: 0.8349\n",
            "Epoch 485/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5923 - accuracy: 0.8358\n",
            "Epoch 486/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5817 - accuracy: 0.8366\n",
            "Epoch 487/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5710 - accuracy: 0.8429\n",
            "Epoch 488/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5821 - accuracy: 0.8357\n",
            "Epoch 489/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5814 - accuracy: 0.8378\n",
            "Epoch 490/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5590 - accuracy: 0.8474\n",
            "Epoch 491/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.5521 - accuracy: 0.8474\n",
            "Epoch 492/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5511 - accuracy: 0.8483\n",
            "Epoch 493/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5529 - accuracy: 0.8461\n",
            "Epoch 494/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5770 - accuracy: 0.8398\n",
            "Epoch 495/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5741 - accuracy: 0.8410\n",
            "Epoch 496/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5600 - accuracy: 0.8442\n",
            "Epoch 497/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.5461 - accuracy: 0.8490\n",
            "Epoch 498/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5574 - accuracy: 0.8454\n",
            "Epoch 499/500\n",
            "290/290 [==============================] - 2s 6ms/step - loss: 0.5923 - accuracy: 0.8376\n",
            "Epoch 500/500\n",
            "290/290 [==============================] - 2s 7ms/step - loss: 0.5867 - accuracy: 0.8352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-rfokivGYvj",
        "outputId": "8d6c6204-3d17-4248-b46e-b3e8bcbff47b"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the\n",
            "[85, 11, 1] [[85 11  1]]\n",
            "Next word suggestion: follow\n",
            "Next word suggestion: rt\n",
            "Next word suggestion: quick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoRzW8O3cF4n",
        "outputId": "58cb0cf9-9b82-47a2-854a-8790fb4beb09"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We need to become a\n",
            "[37, 82, 3, 532, 4] [[  3 532   4]]\n",
            "Next word suggestion: national\n",
            "Next word suggestion: part\n",
            "Next word suggestion: late\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3kXsRjCc2vI",
        "outputId": "4aade7b4-d30e-457c-aae3-817cf0d52abf"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im coming back tomorrow\n",
            "[139, 225, 132, 245] [[225 132 245]]\n",
            "Next word suggestion: night\n",
            "Next word suggestion: would\n",
            "Next word suggestion: my\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCxRz3hDdMhR",
        "outputId": "23a173c2-67e7-4e28-ea72-04f48f1663ae"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i can't wait\n",
            "[2, 242] [[  0   2 242]]\n",
            "Next word suggestion: we\n",
            "Next word suggestion: of\n",
            "Next word suggestion: to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o_iXfGqdgu0",
        "outputId": "1f5719b5-23a6-4941-8008-a51e20999946"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have amazing plans\n",
            "[2, 31, 213, 801] [[ 31 213 801]]\n",
            "Next word suggestion: are\n",
            "Next word suggestion: hope\n",
            "Next word suggestion: like\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHKar3DYd_nm",
        "outputId": "ecf5ba2b-a2fa-47e4-fda5-c865f259475c"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this standup comedy doesn't work out\n",
            "[24, 154, 50] [[ 24 154  50]]\n",
            "Next word suggestion: to\n",
            "Next word suggestion: i\n",
            "Next word suggestion: the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brk0OtgVec0Y",
        "outputId": "5e71273a-32a6-4fb5-88b9-14c8843394d9"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crater Lake is often referred to as the seventh deepest\n",
            "[72, 45, 9, 389, 547, 3, 44, 1, 1086, 175] [[   1 1086  175]]\n",
            "Next word suggestion: lake\n",
            "Next word suggestion: on\n",
            "Next word suggestion: of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm7rYNfnex8v",
        "outputId": "23509bc3-415f-47f7-ccb1-56dee8493396"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U.S. troops  Saudi Arabia\n",
            "[48, 12, 1027, 1028, 1029] [[1027 1028 1029]]\n",
            "Next word suggestion: march\n",
            "Next word suggestion: wrapped\n",
            "Next word suggestion: illegal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f58_hgSpfBJv",
        "outputId": "43be38ef-3a4c-40e5-fdde-d75a2bb49790"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "However\n",
            "[1102] [[   0    0 1102]]\n",
            "Next word suggestion: sellout\n",
            "Next word suggestion: read\n",
            "Next word suggestion: haha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfs0uqwLfb3O",
        "outputId": "f3db42f8-90a7-4072-d136-fcc15a99d1bd"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how many people\n",
            "[17, 22, 21] [[17 22 21]]\n",
            "Next word suggestion: visit\n",
            "Next word suggestion: died\n",
            "Next word suggestion: live\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM_lN61NgTXS",
        "outputId": "5e416081-57ef-4d91-bccf-bb48e40187ca"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks\n",
            "[85] [[ 0  0 85]]\n",
            "Next word suggestion: so\n",
            "Next word suggestion: for\n",
            "Next word suggestion: t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgd_dGeVgbQ7",
        "outputId": "f3eeec34-329a-4828-bc66-e64371a0e901"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for\n",
            "[85, 11] [[ 0 85 11]]\n",
            "Next word suggestion: forgetting\n",
            "Next word suggestion: really\n",
            "Next word suggestion: the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP3vAxZ_gi2Y",
        "outputId": "5cc9a40d-485e-4b49-cb32-431c0df0ce8d"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the\n",
            "[85, 11, 1] [[85 11  1]]\n",
            "Next word suggestion: follow\n",
            "Next word suggestion: rt\n",
            "Next word suggestion: quick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksu8DqlWgt15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNzIy_U-grAP",
        "outputId": "41fbe656-8fb7-43c5-b3cc-aba9f10045db"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick\n",
            "[85, 11, 1, 610] [[ 11   1 610]]\n",
            "Next word suggestion: birthday\n",
            "Next word suggestion: move\n",
            "Next word suggestion: night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zayeD-Mbg3Ab",
        "outputId": "bc9a378f-ac17-4650-e4ff-9c98de7bdde3"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move\n",
            "[85, 11, 1, 610, 611] [[  1 610 611]]\n",
            "Next word suggestion: on\n",
            "Next word suggestion: and\n",
            "Next word suggestion: 2walk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M-SSOnhg_z3",
        "outputId": "9235a2cd-909c-4d79-8b4e-502ccb3ef00b"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on\n",
            "[85, 11, 1, 610, 611, 13] [[610 611  13]]\n",
            "Next word suggestion: status\n",
            "Next word suggestion: bro\n",
            "Next word suggestion: kinda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT0uBOuohHbW",
        "outputId": "3f06f669-2ef4-47fa-e606-19dd3fd3f2f7"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on bro\n",
            "[85, 11, 1, 610, 611, 13, 343] [[611  13 343]]\n",
            "Next word suggestion: intriguing\n",
            "Next word suggestion: shirt\n",
            "Next word suggestion: ah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AMBBlvLhQxU",
        "outputId": "7df98c61-ae91-4595-8ef5-cc5f8454c161"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on bro ah\n",
            "[85, 11, 1, 610, 611, 13, 343, 1681] [[  13  343 1681]]\n",
            "Next word suggestion: i\n",
            "Next word suggestion: music\n",
            "Next word suggestion: that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTcE_vFhc4K",
        "outputId": "17e49d08-7e67-4231-9bc8-e6a777959996"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on bro ah that\n",
            "[85, 11, 1, 610, 611, 13, 343, 1681, 14] [[ 343 1681   14]]\n",
            "Next word suggestion: a\n",
            "Next word suggestion: my\n",
            "Next word suggestion: many\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQg1POUfhrtm",
        "outputId": "d44c57a7-3349-47d7-bb3e-6707fc966803"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on bro ah that many\n",
            "[85, 11, 1, 610, 611, 13, 343, 1681, 14, 22] [[1681   14   22]]\n",
            "Next word suggestion: change\n",
            "Next word suggestion: the\n",
            "Next word suggestion: my\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw7nIbnJh8Sx",
        "outputId": "4cd97979-9e40-4eab-e76b-ad18ac5df4db"
      },
      "source": [
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanks for the quick move on bro ah that many change\n",
            "[85, 11, 1, 610, 611, 13, 343, 1681, 14, 22, 248] [[ 14  22 248]]\n",
            "Next word suggestion: visit\n",
            "Next word suggestion: and\n",
            "Next word suggestion: died\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}